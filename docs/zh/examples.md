---
layout: default
title: ä½¿ç”¨ç¤ºä¾‹
nav_order: 4
description: "LLMåä½œç³»ç»Ÿçš„ç»¼åˆä½¿ç”¨ç¤ºä¾‹å’Œæ•™ç¨‹"
parent: ä¸­æ–‡æ–‡æ¡£
---

# ä½¿ç”¨ç¤ºä¾‹
{: .no_toc }

å±•ç¤ºLLMåä½œç³»ç»Ÿå„ç§ç”¨ä¾‹å’ŒåŠŸèƒ½çš„ç»¼åˆç¤ºä¾‹ã€‚
{: .fs-6 .fw-300 }

## ç›®å½•
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### ç®€å•æ¨ç†

```python
import asyncio
from llm_cooperation import SystemConfig, OpenAIEngine, InferenceRequest

async def basic_inference():
    """åŸºç¡€å•æ¨¡å‹æ¨ç†ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    config = SystemConfig()
    config.update_api_config(
        base_url="https://api2.aigcbest.top/v1",
        api_key="æ‚¨çš„APIå¯†é’¥"
    )
    
    engine = OpenAIEngine(config)
    await engine.initialize()
    
    try:
        # åˆ›å»ºè¯·æ±‚
        request = InferenceRequest(
            prompt="ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šæœºå™¨å­¦ä¹ çš„æ¦‚å¿µ",
            model_name="qwen3_8b",
            max_tokens=300,
            temperature=0.7
        )
        
        # è·å–å“åº”
        response = await engine.inference(request)
        
        if response.success:
            print(f"âœ… æ¨¡å‹: {response.model_name}")
            print(f"ğŸ“ å“åº”:\n{response.text}")
            print(f"â±ï¸ å»¶è¿Ÿ: {response.latency:.2f}ç§’")
            print(f"ğŸ”¢ ä»¤ç‰Œæ•°: {response.usage.get('total_tokens', 'N/A')}")
        else:
            print(f"âŒ é”™è¯¯: {response.error}")
    
    finally:
        await engine.shutdown()

# è¿è¡Œç¤ºä¾‹
asyncio.run(basic_inference())
```

### æ‰¹é‡å¤„ç†

```python
async def batch_processing():
    """æ‰¹é‡å¤„ç†å¤šä¸ªæŸ¥è¯¢"""
    
    config = SystemConfig()
    engine = OpenAIEngine(config)
    await engine.initialize()
    
    # å®šä¹‰æŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        "è§£é‡Šé‡å­è®¡ç®—",
        "åŒºå—é“¾æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æè¿°æœºå™¨å­¦ä¹ ç®—æ³•"
    ]
    
    try:
        # å¤„ç†æ‰€æœ‰æŸ¥è¯¢
        tasks = []
        for i, query in enumerate(queries):
            request = InferenceRequest(
                prompt=query,
                model_name="qwen3_8b",
                max_tokens=150
            )
            tasks.append(engine.inference(request))
        
        # ç­‰å¾…æ‰€æœ‰å“åº”
        responses = await asyncio.gather(*tasks)
        
        # æ˜¾ç¤ºç»“æœ
        for i, (query, response) in enumerate(zip(queries, responses)):
            print(f"\n{'='*50}")
            print(f"æŸ¥è¯¢ {i+1}: {query}")
            print(f"{'='*50}")
            if response.success:
                print(f"å“åº”: {response.text[:200]}...")
                print(f"å»¶è¿Ÿ: {response.latency:.2f}ç§’")
            else:
                print(f"é”™è¯¯: {response.error}")
    
    finally:
        await engine.shutdown()

asyncio.run(batch_processing())
```

## æ™ºèƒ½è·¯ç”±ç¤ºä¾‹

### è‡ªåŠ¨æ¨¡å‹é€‰æ‹©

```python
from llm_cooperation import IntelligentRouter

async def intelligent_routing_demo():
    """æ¼”ç¤ºåŸºäºæŸ¥è¯¢å¤æ‚åº¦çš„è‡ªåŠ¨æ¨¡å‹é€‰æ‹©"""
    
    router = IntelligentRouter()
    
    # ä¸åŒç±»å‹çš„æŸ¥è¯¢
    test_queries = [
        # ç®€å•æŸ¥è¯¢ï¼ˆåº”ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼‰
        ("ç®€å•æ•°å­¦", "15 + 27 ç­‰äºå¤šå°‘ï¼Ÿ"),
        ("åŸºç¡€ç¿»è¯‘", "å°†'æ—©ä¸Šå¥½'ç¿»è¯‘æˆè‹±æ–‡"),
        ("ç®€å•å®šä¹‰", "ä»€ä¹ˆæ˜¯HTTPï¼Ÿ"),
        
        # å¤æ‚æŸ¥è¯¢ï¼ˆåº”ä½¿ç”¨æ¨ç†æ¨¡å‹ï¼‰  
        ("æ•°å­¦è¯æ˜", "è¯æ˜æ ¹å·2æ˜¯æ— ç†æ•°"),
        ("ä»£ç åˆ†æ", "åˆ†æè¿™æ®µPythonä»£ç çš„ä¼˜åŒ–æœºä¼šï¼š\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n```"),
        ("å¤æ‚åˆ†æ", "æ¯”è¾ƒå¤ªé˜³èƒ½å’Œæ ¸èƒ½çš„ç»æµä¸ç¯å¢ƒå½±å“"),
        
        # ä¸­ç­‰å¤æ‚åº¦ï¼ˆå¯èƒ½ä½¿ç”¨åä½œï¼‰
        ("ç ”ç©¶ä»»åŠ¡", "æ€»ç»“é‡å­è®¡ç®—çš„æœ€æ–°å‘å±•"),
        ("åˆ›æ„å†™ä½œ", "å†™ä¸€ä¸ªå…³äºAIä¸äººç±»çš„çŸ­æ•…äº‹"),
    ]
    
    for category, query in test_queries:
        print(f"\nğŸ” {category}")
        print(f"æŸ¥è¯¢: {query[:100]}...")
        print("-" * 50)
        
        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
        analysis = router.analyze_query_complexity(query)
        print(f"ğŸ“Š å¤æ‚åº¦: {analysis['complexity_score']:.2f}")
        print(f"ğŸ·ï¸ ä»»åŠ¡ç±»å‹: {analysis['task_type']}")
        print(f"ğŸ¤– å»ºè®®ç­–ç•¥: {analysis['suggested_strategy']}")
        
        # è·¯ç”±å¹¶è·å–å“åº”
        start_time = time.time()
        result = await router.route_request(query)
        end_time = time.time()
        
        print(f"âš¡ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“ ç»“æœ: {result[:150]}...")

import time
asyncio.run(intelligent_routing_demo())
```

### è‡ªå®šä¹‰è·¯ç”±è§„åˆ™

```python
async def custom_routing_example():
    """è‡ªå®šä¹‰è·¯ç”±é€»è¾‘ç¤ºä¾‹"""
    
    from llm_cooperation.routing import CustomRouter
    
    # å®šä¹‰è‡ªå®šä¹‰è·¯ç”±è§„åˆ™
    routing_rules = {
        "ä»£ç ": {
            "å…³é”®è¯": ["python", "javascript", "ä»£ç ", "å‡½æ•°", "ç±»", "ç®—æ³•"],
            "é¦–é€‰æ¨¡å‹": "qwen3_32b",
            "æœ€å°ä»¤ç‰Œ": 500
        },
        "æ•°å­¦": {
            "å…³é”®è¯": ["æ–¹ç¨‹", "è§£", "è¯æ˜", "å®šç†", "è®¡ç®—"],
            "é¦–é€‰æ¨¡å‹": "qwen3_32b", 
            "æ¸©åº¦": 0.1
        },
        "åˆ›æ„": {
            "å…³é”®è¯": ["æ•…äº‹", "è¯—æ­Œ", "åˆ›æ„", "æƒ³è±¡", "å†™"],
            "é¦–é€‰æ¨¡å‹": "qwen3_8b",
            "æ¸©åº¦": 0.9
        },
        "ç¿»è¯‘": {
            "å…³é”®è¯": ["ç¿»è¯‘", "translation", "è¯­è¨€"],
            "é¦–é€‰æ¨¡å‹": "qwen3_8b",
            "æœ€å¤§ä»¤ç‰Œ": 200
        }
    }
    
    router = CustomRouter(routing_rules)
    
    test_cases = [
        "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°",
        "è§£æ–¹ç¨‹: 2xÂ² + 5x - 3 = 0", 
        "å†™ä¸€ä¸ªå…³äºå¤ªç©ºæ¢ç´¢çš„åˆ›æ„æ•…äº‹",
        "å°†è¿™å¥è¯ç¿»è¯‘æˆè¥¿ç­ç‰™è¯­: 'ä»Šå¤©å¤©æ°”å¾ˆå¥½'"
    ]
    
    for query in test_cases:
        route_info = router.determine_route(query)
        print(f"\næŸ¥è¯¢: {query}")
        print(f"æ£€æµ‹åˆ°çš„ç±»åˆ«: {route_info['category']}")
        print(f"é€‰æ‹©çš„æ¨¡å‹: {route_info['model']}")
        print(f"å‚æ•°: {route_info['params']}")
        
        # ä½¿ç”¨ç¡®å®šçš„è·¯ç”±æ‰§è¡Œ
        result = await router.execute_with_routing(query, route_info)
        print(f"ç»“æœ: {result[:100]}...")

asyncio.run(custom_routing_example())
```

## å¤šæ¨¡å‹åä½œç¤ºä¾‹

### é¡ºåºåä½œ

```python
from llm_cooperation import CooperationScheduler

async def sequential_cooperation_example():
    """é¡ºåºæ¨¡å‹åä½œç¤ºä¾‹"""
    
    scheduler = CooperationScheduler()
    
    # å¤æ‚åˆ†æä»»åŠ¡
    complex_query = """
    åˆ†æäººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰å¯¹ç¤¾ä¼šçš„æ½œåœ¨å½±å“ã€‚
    è€ƒè™‘æŠ€æœ¯ã€ç»æµã€ç¤¾ä¼šå’Œä¼¦ç†ç»´åº¦ã€‚
    ä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›å…·ä½“å»ºè®®ã€‚
    """
    
    print("ğŸ”„ é¡ºåºåä½œç¤ºä¾‹")
    print("=" * 50)
    print(f"æŸ¥è¯¢: {complex_query[:100]}...")
    
    # æ­¥éª¤1ï¼šä½¿ç”¨æ¨ç†æ¨¡å‹è¿›è¡Œåˆæ­¥åˆ†æ
    print("\nğŸ“‹ æ­¥éª¤1: æ·±åº¦åˆ†æï¼ˆGPT-4çº§åˆ«æ¨¡å‹ï¼‰")
    result_step1 = await scheduler.create_sequential_task(
        query=complex_query,
        models=["qwen3_32b"],  # ä»å¼ºå¤§çš„æ¨¡å‹å¼€å§‹
        integration_strategy="detailed_analysis"
    )
    
    # æ­¥éª¤2ï¼šä¼˜åŒ–å’Œç»“æ„åŒ–
    print("\nğŸ“‹ æ­¥éª¤2: ä¼˜åŒ–å’Œç»„ç»‡")
    refinement_query = f"""
    åŸºäºè¿™ä¸ªåˆ†æ: {result_step1}
    
    è¯·ï¼š
    1. å°†å†…å®¹ç»„ç»‡æˆæ¸…æ™°çš„éƒ¨åˆ†
    2. æ·»åŠ å…·ä½“çš„ä¾‹å­å’Œæ¡ˆä¾‹ç ”ç©¶
    3. æä¾›å¯æ“ä½œçš„å»ºè®®
    4. ç¡®ä¿æ¸…æ™°åº¦å’Œå¯è®¿é—®æ€§
    """
    
    final_result = await scheduler.create_sequential_task(
        query=refinement_query,
        models=["qwen3_8b"],  # ä½¿ç”¨é«˜æ•ˆæ¨¡å‹è¿›è¡Œç»„ç»‡
        integration_strategy="structured_output"
    )
    
    print(f"\nâœ… æœ€ç»ˆç»“æœ:\n{final_result}")
    print(f"\nğŸ“Š æ€»å¤„ç†æ¶‰åŠ2ä¸ªæ¨¡å‹çš„é¡ºåºåä½œ")

asyncio.run(sequential_cooperation_example())
```

### å¹¶è¡Œåä½œä¸æŠ•ç¥¨

```python
async def parallel_voting_example():
    """å¹¶è¡Œåä½œä¸æŠ•ç¥¨æœºåˆ¶ç¤ºä¾‹"""
    
    scheduler = CooperationScheduler()
    
    # å—ç›Šäºå¤šä¸ªè§†è§’çš„é—®é¢˜
    question = """
    æœªæ¥åå¹´æœ€æœ‰å‰æ™¯çš„å¯å†ç”Ÿèƒ½æºæŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ
    è€ƒè™‘æ•ˆç‡ã€æˆæœ¬ã€å¯æ‰©å±•æ€§å’Œç¯å¢ƒå½±å“ã€‚
    """
    
    print("ğŸ—³ï¸ å¹¶è¡Œåä½œä¸æŠ•ç¥¨ç¤ºä¾‹")
    print("=" * 50)
    print(f"é—®é¢˜: {question}")
    
    # ä»å¤šä¸ªæ¨¡å‹å¹¶è¡Œè·å–å“åº”
    print("\nğŸ“Š ä»å¤šä¸ªæ¨¡å‹è·å–å“åº”...")
    
    result = await scheduler.create_parallel_task(
        query=question,
        models=["qwen3_32b", "qwen3_8b", "qwen3_32b"],  # ä½¿ç”¨å¤šä¸ªå®ä¾‹
        integration_strategy="voting",
        voting_params={
            "consensus_threshold": 0.6,
            "weight_by_confidence": True,
            "include_dissenting_views": True
        }
    )
    
    print(f"\nâœ… å…±è¯†ç»“æœ:\n{result['consensus']}")
    
    if result.get('dissenting_views'):
        print(f"\nğŸ¤” ä¸åŒè§‚ç‚¹:\n{result['dissenting_views']}")
    
    print(f"\nğŸ“ˆ ç½®ä¿¡åº¦åˆ†æ•°: {result['confidence_score']:.2f}")

asyncio.run(parallel_voting_example())
```

### æµæ°´çº¿åä½œ

```python
async def pipeline_cooperation_example():
    """åŸºäºæµæ°´çº¿çš„æ¨¡å‹åä½œç¤ºä¾‹"""
    
    scheduler = CooperationScheduler()
    
    # æ–‡æ¡£å¤„ç†æµæ°´çº¿
    document = """
    äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨è¿…é€Ÿæ”¹å˜å…¨çƒå„è¡Œå„ä¸šã€‚
    ä»åŒ»ç–—è¯Šæ–­åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ŒAIåº”ç”¨å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚
    ç„¶è€Œï¼Œè¿™é¡¹æŠ€æœ¯ä¹Ÿæå‡ºäº†å…³äºå°±ä¸šã€éšç§å’Œä¼¦ç†è€ƒè™‘çš„é‡è¦é—®é¢˜ã€‚
    å…¬å¸æ­£åœ¨AIç ”ç©¶ä¸ŠæŠ•èµ„æ•°åäº¿ç¾å…ƒï¼Œè€Œæ”¿åºœæ­£åœ¨åŠªåŠ›åˆ¶å®šç›‘ç®¡æ¡†æ¶ã€‚
    AIçš„æœªæ¥å°†å–å†³äºæˆ‘ä»¬å¦‚ä½•å¹³è¡¡åˆ›æ–°ä¸è´Ÿè´£ä»»çš„å‘å±•ã€‚
    """
    
    print("ğŸ”„ æµæ°´çº¿åä½œç¤ºä¾‹")
    print("=" * 50)
    
    # å®šä¹‰å¤„ç†æµæ°´çº¿
    pipeline_config = [
        {
            "step": "extraction",
            "model": "qwen3_8b",
            "task": "ä»æ–‡æ¡£ä¸­æå–å…³é”®äº‹å®å’Œç»Ÿè®¡æ•°æ®",
            "params": {"max_tokens": 300}
        },
        {
            "step": "analysis", 
            "model": "qwen3_32b",
            "task": "åŸºäºæå–çš„äº‹å®åˆ†æå½±å“å¹¶æä¾›æ´å¯Ÿ",
            "params": {"max_tokens": 400, "temperature": 0.3}
        },
        {
            "step": "summary",
            "model": "qwen3_8b", 
            "task": "ç»“åˆäº‹å®å’Œåˆ†æåˆ›å»ºæ‰§è¡Œæ‘˜è¦",
            "params": {"max_tokens": 200, "temperature": 0.5}
        },
        {
            "step": "recommendations",
            "model": "qwen3_32b",
            "task": "åŸºäºåˆ†ææä¾›å¯æ“ä½œçš„å»ºè®®",
            "params": {"max_tokens": 300, "temperature": 0.4}
        }
    ]
    
    # æ‰§è¡Œæµæ°´çº¿
    result = await scheduler.create_pipeline_task(
        query=document,
        pipeline_config=pipeline_config
    )
    
    # æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„ç»“æœ
    for step_name, step_result in result['pipeline_results'].items():
        print(f"\nğŸ“‹ {step_name.upper()}:")
        print("-" * 30)
        print(step_result['output'][:300] + "...")
        print(f"â±ï¸ æ­¥éª¤æ—¶é—´: {step_result['processing_time']:.2f}ç§’")
    
    print(f"\nâœ… æœ€ç»ˆæ•´åˆç»“æœ:")
    print("=" * 50)
    print(result['final_result'])
    
    print(f"\nğŸ“Š æµæ°´çº¿ç»Ÿè®¡:")
    print(f"æ€»æ­¥éª¤æ•°: {len(pipeline_config)}")
    print(f"æ€»æ—¶é—´: {result['total_time']:.2f}ç§’")
    print(f"ä½¿ç”¨çš„æ¨¡å‹: {', '.join(set([step['model'] for step in pipeline_config]))}")

asyncio.run(pipeline_cooperation_example())
```

## ä¼ä¸šåº”ç”¨ç¤ºä¾‹

### æ–‡æ¡£åˆ†ææœåŠ¡

```python
from llm_cooperation import ApplicationServiceManager

async def document_analysis_example():
    """ä¼ä¸šæ–‡æ¡£åˆ†æç¤ºä¾‹"""
    
    service_manager = ApplicationServiceManager()
    
    # æ ·æœ¬å•†ä¸šæ–‡æ¡£
    business_report = """
    2024å¹´ç¬¬ä¸‰å­£åº¦è´¢åŠ¡è¡¨ç°æŠ¥å‘Š
    
    æ‰§è¡Œæ‘˜è¦ï¼š
    æˆ‘ä»¬å…¬å¸åœ¨2024å¹´ç¬¬ä¸‰å­£åº¦å®ç°äº†åˆ›çºªå½•çš„1250ä¸‡ç¾å…ƒæ”¶å…¥ï¼Œ
    æ¯”2023å¹´ç¬¬ä¸‰å­£åº¦å¢é•¿35%ã€‚å®¢æˆ·è·å–å¢é•¿42%ï¼Œæ–°å¢2100å®¶ä¼ä¸šå®¢æˆ·ã€‚
    ç„¶è€Œï¼Œç”±äºæ‰©å¤§çš„ç ”å‘æŠ•èµ„ï¼Œè¿è¥è´¹ç”¨å¢åŠ äº†28%ã€‚
    
    å…³é”®æŒ‡æ ‡ï¼š
    - æ”¶å…¥: 1250ä¸‡ç¾å…ƒï¼ˆåŒæ¯”å¢é•¿35%ï¼‰
    - æ¯›åˆ©ç‡: 68%ï¼ˆåŒæ¯”å¢é•¿3%ï¼‰
    - å®¢æˆ·æ•°é‡: 7200ï¼ˆåŒæ¯”å¢é•¿42%ï¼‰
    - å‘˜å·¥æ•°é‡: 145ï¼ˆåŒæ¯”å¢é•¿15%ï¼‰
    - ç ”å‘æ”¯å‡º: 210ä¸‡ç¾å…ƒï¼ˆåŒæ¯”å¢é•¿45%ï¼‰
    
    æŒ‘æˆ˜ï¼š
    - ä¾›åº”é“¾ä¸­æ–­å½±å“äº†15%çš„è®¢å•
    - æ ¸å¿ƒå¸‚åœºç«äº‰åŠ å‰§
    - äººæ‰è·å–æˆæœ¬å¢åŠ 30%
    
    æœºé‡ï¼š
    - è®¡åˆ’ç¬¬å››å­£åº¦æ¨å‡ºæ–°äº§å“
    - è·æ‰¹è¿›å…¥æ¬§æ´²å¸‚åœºæ‰©å¼ 
    - ä¸TechCorpç­¾ç½²æˆ˜ç•¥åˆä½œä¼™ä¼´å…³ç³»
    """
    
    print("ğŸ“„ æ–‡æ¡£åˆ†ææœåŠ¡ç¤ºä¾‹")
    print("=" * 50)
    
    # ç»¼åˆåˆ†æ
    analysis_response = await service_manager.process_request(
        service_type="document_analysis",
        content=business_report,
        parameters={
            "analysis_type": "comprehensive",
            "include_metrics": True,
            "include_recommendations": True,
            "output_format": "structured"
        }
    )
    
    if analysis_response.success:
        result = analysis_response.result
        
        print("ğŸ“Š å…³é”®æ´å¯Ÿ:")
        for insight in result.get('key_insights', []):
            print(f"  â€¢ {insight}")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        for metric in result.get('extracted_metrics', []):
            print(f"  â€¢ {metric}")
        
        print(f"\nâš ï¸ é£é™©å› ç´ :")
        for risk in result.get('risk_factors', []):
            print(f"  â€¢ {risk}")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in result.get('recommendations', []):
            print(f"  â€¢ {rec}")
        
        print(f"\nâ±ï¸ åˆ†æåœ¨{analysis_response.processing_time:.2f}ç§’å†…å®Œæˆ")
        print(f"ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹: {analysis_response.model_used}")
    
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {analysis_response.error}")

asyncio.run(document_analysis_example())
```

### æ•°æ®æ´å¯ŸæœåŠ¡

```python
async def data_insights_example():
    """æ•°æ®åˆ†æå’Œæ´å¯Ÿç”Ÿæˆç¤ºä¾‹"""
    
    service_manager = ApplicationServiceManager()
    
    # æ ·æœ¬é”€å”®æ•°æ®
    sales_data = """
    æœˆåº¦é”€å”®æ•°æ®ï¼ˆ2024å¹´ï¼‰ï¼š
    ä¸€æœˆ: 85ä¸‡å…ƒï¼ˆ650ä¸ªå®¢æˆ·ï¼‰
    äºŒæœˆ: 92ä¸‡å…ƒï¼ˆ720ä¸ªå®¢æˆ·ï¼‰
    ä¸‰æœˆ: 110ä¸‡å…ƒï¼ˆ850ä¸ªå®¢æˆ·ï¼‰
    å››æœˆ: 98ä¸‡å…ƒï¼ˆ780ä¸ªå®¢æˆ·ï¼‰
    äº”æœˆ: 120ä¸‡å…ƒï¼ˆ920ä¸ªå®¢æˆ·ï¼‰
    å…­æœˆ: 135ä¸‡å…ƒï¼ˆ1050ä¸ªå®¢æˆ·ï¼‰
    ä¸ƒæœˆ: 110ä¸‡å…ƒï¼ˆ880ä¸ªå®¢æˆ·ï¼‰
    å…«æœˆ: 145ä¸‡å…ƒï¼ˆ1100ä¸ªå®¢æˆ·ï¼‰
    ä¹æœˆ: 160ä¸‡å…ƒï¼ˆ1200ä¸ªå®¢æˆ·ï¼‰
    
    äº§å“ç±»åˆ«ï¼š
    - è½¯ä»¶è®¸å¯: æ”¶å…¥çš„45%
    - ä¸“ä¸šæœåŠ¡: æ”¶å…¥çš„30%
    - æ”¯æŒä¸ç»´æŠ¤: æ”¶å…¥çš„20%
    - åŸ¹è®­: æ”¶å…¥çš„5%
    
    åœ°ç†åˆ†å¸ƒï¼š
    - åŒ—ç¾: 60%
    - æ¬§æ´²: 25%
    - äºšå¤ª: 12%
    - å…¶ä»–: 3%
    """
    
    print("ğŸ“Š æ•°æ®æ´å¯ŸæœåŠ¡ç¤ºä¾‹")
    print("=" * 50)
    
    # è¶‹åŠ¿åˆ†æ
    trend_response = await service_manager.process_request(
        service_type="data_insight",
        content=sales_data,
        parameters={
            "insight_type": "trends",
            "include_forecasting": True,
            "include_correlations": True,
            "confidence_intervals": True
        }
    )
    
    if trend_response.success:
        result = trend_response.result
        
        print("ğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
        print(f"  æ€»ä½“è¶‹åŠ¿: {result.get('overall_trend')}")
        print(f"  å¢é•¿ç‡: {result.get('growth_rate')}")
        print(f"  å­£èŠ‚æ€§: {result.get('seasonality_pattern')}")
        
        print(f"\nğŸ”® é¢„æµ‹:")
        for period, forecast in result.get('forecasts', {}).items():
            print(f"  {period}: {forecast}")
        
        print(f"\nğŸ”— å…³é”®ç›¸å…³æ€§:")
        for correlation in result.get('correlations', []):
            print(f"  â€¢ {correlation}")
        
        print(f"\nğŸ’¡ æˆ˜ç•¥æ´å¯Ÿ:")
        for insight in result.get('strategic_insights', []):
            print(f"  â€¢ {insight}")
    
    # æ¯”è¾ƒåˆ†æ
    print(f"\n{'='*50}")
    print("ğŸ” æ¯”è¾ƒåˆ†æ")
    
    comparison_response = await service_manager.process_request(
        service_type="data_insight",
        content=sales_data,
        parameters={
            "insight_type": "comparative",
            "compare_periods": ["Q1", "Q2", "Q3"],
            "include_benchmarks": True
        }
    )
    
    if comparison_response.success:
        comp_result = comparison_response.result
        
        print("ğŸ“Š å­£åº¦æ¯”è¾ƒ:")
        for quarter, metrics in comp_result.get('quarterly_breakdown', {}).items():
            print(f"  {quarter}: {metrics}")
        
        print(f"\nğŸ¯ æ€§èƒ½åŸºå‡†:")
        for benchmark in comp_result.get('benchmarks', []):
            print(f"  â€¢ {benchmark}")

asyncio.run(data_insights_example())
```

### å†³ç­–æ”¯æŒç³»ç»Ÿ

```python
async def decision_support_example():
    """æˆ˜ç•¥å†³ç­–æ”¯æŒç¤ºä¾‹"""
    
    service_manager = ApplicationServiceManager()
    
    # å•†ä¸šåœºæ™¯
    decision_scenario = """
    æˆ˜ç•¥å†³ç­–ï¼šå¸‚åœºæ‰©å¼ 
    
    èƒŒæ™¯ï¼š
    æˆ‘ä»¬çš„SaaSå…¬å¸ï¼ˆå½“å‰æ”¶å…¥ï¼š1500ä¸‡å…ƒ/å¹´ï¼‰æ­£åœ¨è€ƒè™‘è¿›å…¥æ¬§æ´²å¸‚åœºã€‚
    æˆ‘ä»¬åœ¨åŒ—ç¾æœ‰å¼ºå¤§çš„äº§å“å¸‚åœºåŒ¹é…ï¼Œå®¢æˆ·æ»¡æ„åº¦85%ï¼Œæœˆæµå¤±ç‡15%ã€‚
    
    æ‰©å¼ é€‰é¡¹ï¼š
    1. ç›´é”€æ–¹å¼ï¼šåœ¨è‹±å›½ã€å¾·å›½ã€æ³•å›½é›‡ä½£æœ¬åœ°é”€å”®å›¢é˜Ÿ
       - é¢„ä¼°æŠ•èµ„ï¼šç¬¬ä¸€å¹´250ä¸‡å…ƒ
       - é¢„è®¡æ”¶å…¥ï¼šç¬¬äºŒå¹´åº•300-500ä¸‡å…ƒ
       - é£é™©çº§åˆ«ï¼šä¸­é«˜
    
    2. åˆä½œä¼™ä¼´æ¸ é“ç­–ç•¥ï¼šä¸æœ¬åœ°ç³»ç»Ÿé›†æˆå•†åˆä½œ
       - é¢„ä¼°æŠ•èµ„ï¼šç¬¬ä¸€å¹´80ä¸‡å…ƒ
       - é¢„è®¡æ”¶å…¥ï¼šç¬¬äºŒå¹´åº•150-300ä¸‡å…ƒ
       - é£é™©çº§åˆ«ï¼šä¸­ç­‰
    
    3. æ”¶è´­ç­–ç•¥ï¼šæ”¶è´­æ¬§æ´²ç«äº‰å¯¹æ‰‹
       - é¢„ä¼°æŠ•èµ„ï¼š800-1200ä¸‡å…ƒ
       - é¢„è®¡æ”¶å…¥ï¼š400-600ä¸‡å…ƒç«‹å³+å¢é•¿
       - é£é™©çº§åˆ«ï¼šé«˜
    
    çº¦æŸæ¡ä»¶ï¼š
    - å¯ç”¨èµ„æœ¬ï¼š1000ä¸‡å…ƒ
    - æ—¶é—´çº¿ï¼šå¿…é¡»åœ¨18ä¸ªæœˆå†…æ˜¾ç¤ºç»“æœ
    - å½“å‰å›¢é˜Ÿï¼š45åå‘˜å·¥ï¼ˆå…¶ä¸­8åé”€å”®ï¼‰
    - ç›‘ç®¡ï¼šéœ€è¦GDPRåˆè§„
    
    å¸‚åœºæƒ…æŠ¥ï¼š
    - æ¬§æ´²SaaSå¸‚åœºå¹´å¢é•¿22%
    - å·²æœ‰3ä¸ªä¸»è¦ç«äº‰å¯¹æ‰‹
    - å¹³å‡å®¢æˆ·è·å–æˆæœ¬ï¼š1200æ¬§å…ƒ
    - å¹³å‡äº¤æ˜“è§„æ¨¡ï¼šå¹´è´¹35000æ¬§å…ƒ
    """
    
    print("ğŸ¯ å†³ç­–æ”¯æŒç³»ç»Ÿç¤ºä¾‹")
    print("=" * 50)
    
    # ç»¼åˆå†³ç­–åˆ†æ
    decision_response = await service_manager.process_request(
        service_type="decision_support",
        content=decision_scenario,
        parameters={
            "analysis_framework": "strategic_options",
            "include_risk_assessment": True,
            "include_financial_modeling": True,
            "decision_criteria": ["ROI", "risk", "timeline", "strategic_fit"],
            "recommendation_confidence": True
        }
    )
    
    if decision_response.success:
        result = decision_response.result
        
        print("âš–ï¸ å†³ç­–åˆ†æ:")
        print(f"æ¨èé€‰é¡¹: {result.get('recommended_option')}")
        print(f"ç½®ä¿¡åº¦: {result.get('confidence_score', 0)*100:.1f}%")
        
        print(f"\nğŸ“Š é€‰é¡¹æ¯”è¾ƒ:")
        for option, analysis in result.get('option_analysis', {}).items():
            print(f"\n{option}:")
            print(f"  ä¼˜ç‚¹: {', '.join(analysis.get('pros', []))}")
            print(f"  ç¼ºç‚¹: {', '.join(analysis.get('cons', []))}")
            print(f"  é£é™©åˆ†æ•°: {analysis.get('risk_score', 'N/A')}/10")
            print(f"  é¢„æœŸROI: {analysis.get('expected_roi', 'N/A')}")
        
        print(f"\nğŸ¯ å…³é”®æˆåŠŸå› ç´ :")
        for factor in result.get('success_factors', []):
            print(f"  â€¢ {factor}")
        
        print(f"\nâš ï¸ å…³é”®é£é™©:")
        for risk in result.get('critical_risks', []):
            print(f"  â€¢ {risk['description']} (å½±å“: {risk['impact']}, æ¦‚ç‡: {risk['probability']})")
        
        print(f"\nğŸ“‹ å®æ–½è·¯çº¿å›¾:")
        for phase in result.get('implementation_plan', []):
            print(f"  é˜¶æ®µ{phase['phase']}: {phase['description']}")
            print(f"    æ—¶é—´çº¿: {phase['timeline']}")
            print(f"    èµ„æº: {phase['resources']}")
        
        print(f"\nğŸ“ˆ è´¢åŠ¡é¢„æµ‹:")
        for projection in result.get('financial_projections', []):
            print(f"  â€¢ {projection}")
    
    else:
        print(f"âŒ å†³ç­–åˆ†æå¤±è´¥: {decision_response.error}")

asyncio.run(decision_support_example())
```

## é«˜çº§é…ç½®ç¤ºä¾‹

### åŠ¨æ€æ¨¡å‹ç®¡ç†

```python
from llm_cooperation.tools import APIConfigManager

async def dynamic_model_management():
    """åŠ¨æ€æ¨¡å‹é…ç½®å’Œç®¡ç†ç¤ºä¾‹"""
    
    config_manager = APIConfigManager()
    
    print("âš™ï¸ åŠ¨æ€æ¨¡å‹ç®¡ç†ç¤ºä¾‹")
    print("=" * 50)
    
    # æ·»åŠ å¤šä¸ªAPIæä¾›å•†
    providers = [
        {
            "name": "aigc_best",
            "base_url": "https://api2.aigcbest.top/v1",
            "api_key": "æ‚¨çš„aigcå¯†é’¥",
            "models": ["Qwen/Qwen3-32B", "Qwen/Qwen3-8B", "DeepSeek-V3"]
        },
        {
            "name": "openai", 
            "base_url": "https://api.openai.com/v1",
            "api_key": "æ‚¨çš„openaiå¯†é’¥",
            "models": ["gpt-4", "gpt-3.5-turbo"]
        },
        {
            "name": "custom_provider",
            "base_url": "https://api.custom.com/v1", 
            "api_key": "æ‚¨çš„è‡ªå®šä¹‰å¯†é’¥",
            "models": ["custom-model-1", "custom-model-2"]
        }
    ]
    
    # é…ç½®æä¾›å•†
    for provider in providers:
        print(f"\nğŸ”§ é…ç½® {provider['name']}...")
        
        config_manager.add_endpoint_config(
            name=provider['name'],
            base_url=provider['base_url'],
            api_key=provider['api_key']
        )
        
        # ä¸ºæ­¤æä¾›å•†æ·»åŠ æ¨¡å‹
        for model in provider['models']:
            model_name = f"{provider['name']}_{model.split('/')[-1].lower().replace('-', '_')}"
            config_manager.add_model_config(
                model_name=model_name,
                model_path=model,
                provider=provider['name'],
                supported_tasks=["text", "chat", "completion"]
            )
            print(f"  âœ… æ·»åŠ æ¨¡å‹: {model_name} -> {model}")
    
    # æµ‹è¯•è¿æ¥
    print(f"\nğŸ” æµ‹è¯•æ¨¡å‹è¿æ¥...")
    connectivity_results = await config_manager.test_model_connectivity()
    
    for model, result in connectivity_results.items():
        status = "âœ…" if result['accessible'] else "âŒ"
        latency = f"{result['latency']:.0f}ms" if result.get('latency') else "N/A"
        print(f"  {status} {model}: {latency}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print(f"\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    benchmark_query = "æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    for model_name in ["aigc_best_qwen3_8b", "openai_gpt_3_5_turbo"]:
        if connectivity_results.get(model_name, {}).get('accessible'):
            start_time = time.time()
            try:
                # æ¨¡æ‹Ÿæ¨ç†ï¼ˆå®é™…åœºæ™¯ä¸­ä¼šä½¿ç”¨çœŸå®å¼•æ“ï¼‰
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå“åº”æ—¶é—´
                end_time = time.time()
                print(f"  ğŸ“ˆ {model_name}: {(end_time-start_time)*1000:.0f}ms")
            except Exception as e:
                print(f"  âŒ {model_name}: é”™è¯¯ - {e}")
    
    # åŠ¨æ€è´Ÿè½½å‡è¡¡é…ç½®
    print(f"\nâš–ï¸ è®¾ç½®è´Ÿè½½å‡è¡¡...")
    load_balance_config = {
        "lightweight_tasks": {
            "models": ["aigc_best_qwen3_8b", "openai_gpt_3_5_turbo"],
            "strategy": "round_robin",
            "health_check_interval": 30
        },
        "reasoning_tasks": {
            "models": ["aigc_best_qwen3_32b", "openai_gpt_4"],
            "strategy": "least_latency",
            "fallback_models": ["aigc_best_qwen3_8b"]
        }
    }
    
    config_manager.configure_load_balancing(load_balance_config)
    print("  âœ… è´Ÿè½½å‡è¡¡å·²é…ç½®")
    
    # æ¨¡å‹å¥åº·ç›‘æ§
    print(f"\nğŸ’“ å¥åº·ç›‘æ§è®¾ç½®...")
    health_config = {
        "check_interval": 60,  # ç§’
        "failure_threshold": 3,
        "recovery_threshold": 2,
        "alert_webhooks": ["https://alerts.company.com/webhook"]
    }
    
    config_manager.setup_health_monitoring(health_config)
    print("  âœ… å¥åº·ç›‘æ§å·²æ¿€æ´»")

import time
asyncio.run(dynamic_model_management())
```

### æ€§èƒ½ç›‘æ§

```python
from llm_cooperation.monitoring import PerformanceMonitor
import matplotlib.pyplot as plt
import pandas as pd

async def performance_monitoring_example():
    """ç»¼åˆæ€§èƒ½ç›‘æ§ç¤ºä¾‹"""
    
    monitor = PerformanceMonitor()
    
    print("ğŸ“Š æ€§èƒ½ç›‘æ§ç¤ºä¾‹")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿå„ç§è¯·æ±‚è¿›è¡Œç›‘æ§
    test_scenarios = [
        {"type": "simple", "model": "qwen3_8b", "tokens": 100},
        {"type": "complex", "model": "qwen3_32b", "tokens": 500},
        {"type": "batch", "model": "qwen3_8b", "tokens": 200},
        {"type": "cooperation", "model": "multi", "tokens": 800},
    ]
    
    # æ”¶é›†æ€§èƒ½æ•°æ®
    performance_data = []
    
    for scenario in test_scenarios * 10:  # æ¯ç§åœºæ™¯è¿è¡Œ10æ¬¡
        start_time = time.time()
        
        # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
        processing_time = {
            "simple": 0.5 + random.uniform(-0.2, 0.3),
            "complex": 2.0 + random.uniform(-0.5, 1.0), 
            "batch": 1.2 + random.uniform(-0.3, 0.5),
            "cooperation": 3.5 + random.uniform(-1.0, 2.0)
        }[scenario["type"]]
        
        await asyncio.sleep(processing_time)
        end_time = time.time()
        
        # è®°å½•æŒ‡æ ‡
        metrics = {
            "timestamp": time.time(),
            "request_type": scenario["type"],
            "model": scenario["model"],
            "tokens": scenario["tokens"],
            "latency": end_time - start_time,
            "success": random.choice([True, True, True, False]),  # 75%æˆåŠŸç‡
            "memory_usage": random.uniform(100, 500),  # MB
            "cpu_usage": random.uniform(20, 80)  # %
        }
        
        performance_data.append(metrics)
        await monitor.record_metrics(metrics)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print(f"\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
    df = pd.DataFrame(performance_data)
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"æ€»è¯·æ±‚æ•°: {len(df)}")
    print(f"æˆåŠŸç‡: {(df['success'].sum() / len(df) * 100):.1f}%")
    print(f"å¹³å‡å»¶è¿Ÿ: {df['latency'].mean():.2f}ç§’")
    print(f"95%å»¶è¿Ÿ: {df['latency'].quantile(0.95):.2f}ç§’")
    
    # æŒ‰è¯·æ±‚ç±»å‹åˆ†ç»„
    print(f"\nğŸ“Š æŒ‰è¯·æ±‚ç±»å‹:")
    type_stats = df.groupby('request_type').agg({
        'latency': ['mean', 'std', 'count'],
        'success': 'mean',
        'tokens': 'mean'
    }).round(2)
    
    for req_type in type_stats.index:
        stats = type_stats.loc[req_type]
        print(f"  {req_type.upper()}:")
        print(f"    å¹³å‡å»¶è¿Ÿ: {stats[('latency', 'mean')]:.2f}ç§’ Â± {stats[('latency', 'std')]:.2f}ç§’")
        print(f"    æˆåŠŸç‡: {stats[('success', 'mean')]*100:.1f}%")
        print(f"    è¯·æ±‚æ•°: {stats[('latency', 'count')]}")
    
    # æ€§èƒ½å‘Šè­¦
    print(f"\nğŸš¨ æ€§èƒ½å‘Šè­¦:")
    alerts = await monitor.check_performance_thresholds({
        "max_latency": 3.0,
        "min_success_rate": 0.90,
        "max_error_rate": 0.10
    })
    
    for alert in alerts:
        print(f"  âš ï¸ {alert['severity'].upper()}: {alert['message']}")
    
    # èµ„æºåˆ©ç”¨ç‡
    print(f"\nğŸ’¾ èµ„æºåˆ©ç”¨ç‡:")
    print(f"å¹³å‡å†…å­˜ä½¿ç”¨: {df['memory_usage'].mean():.1f} MB")
    print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {df['memory_usage'].max():.1f} MB")
    print(f"å¹³å‡CPUä½¿ç”¨: {df['cpu_usage'].mean():.1f}%")
    print(f"å³°å€¼CPUä½¿ç”¨: {df['cpu_usage'].max():.1f}%")
    
    # è¶‹åŠ¿åˆ†æ
    print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
    recent_data = df.tail(20)  # æœ€è¿‘20ä¸ªè¯·æ±‚
    older_data = df.head(20)   # å‰20ä¸ªè¯·æ±‚
    
    latency_trend = recent_data['latency'].mean() - older_data['latency'].mean()
    success_trend = recent_data['success'].mean() - older_data['success'].mean()
    
    print(f"å»¶è¿Ÿè¶‹åŠ¿: {'â†—ï¸ +' if latency_trend > 0 else 'â†˜ï¸ '}{latency_trend:.2f}ç§’")
    print(f"æˆåŠŸç‡è¶‹åŠ¿: {'â†—ï¸ +' if success_trend > 0 else 'â†˜ï¸ '}{success_trend*100:.1f}%")

import random
import time
asyncio.run(performance_monitoring_example())
```

## é”™è¯¯å¤„ç†å’Œå®¹é”™ç¤ºä¾‹

### ç»¼åˆé”™è¯¯å¤„ç†

```python
from llm_cooperation.exceptions import *
import logging

async def error_handling_example():
    """ç»¼åˆé”™è¯¯å¤„ç†å’Œå®¹é”™ç¤ºä¾‹"""
    
    print("ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œå®¹é”™ç¤ºä¾‹")
    print("=" * 50)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    config = SystemConfig()
    engine = OpenAIEngine(config)
    
    # å¯èƒ½å¤±è´¥çš„æµ‹è¯•åœºæ™¯
    error_scenarios = [
        {
            "name": "æ— æ•ˆæ¨¡å‹",
            "request": InferenceRequest(
                prompt="æµ‹è¯•", 
                model_name="nonexistent_model"
            ),
            "expected_error": ModelNotFoundError
        },
        {
            "name": "ç©ºæç¤º", 
            "request": InferenceRequest(
                prompt="",
                model_name="qwen3_8b"
            ),
            "expected_error": ValidationError
        },
        {
            "name": "è¿‡åº¦ä»¤ç‰Œè¯·æ±‚",
            "request": InferenceRequest(
                prompt="æµ‹è¯•" * 1000,
                model_name="qwen3_8b", 
                max_tokens=50000  # ä¸ç°å®çš„æ•°å­—
            ),
            "expected_error": APILimitError
        }
    ]
    
    try:
        await engine.initialize()
        
        for scenario in error_scenarios:
            print(f"\nğŸ§ª æµ‹è¯•: {scenario['name']}")
            print("-" * 30)
            
            try:
                response = await engine.inference(scenario['request'])
                
                if not response.success:
                    print(f"âœ… é¢„æœŸé”™è¯¯å·²ä¼˜é›…å¤„ç†: {response.error}")
                else:
                    print(f"âš ï¸ æ„å¤–æˆåŠŸ: {response.text[:50]}...")
                    
            except scenario['expected_error'] as e:
                print(f"âœ… æ•è·é¢„æœŸé”™è¯¯: {type(e).__name__}: {e}")
            except Exception as e:
                print(f"âŒ æ„å¤–é”™è¯¯: {type(e).__name__}: {e}")
                logger.error(f"åœºæ™¯{scenario['name']}ä¸­çš„æ„å¤–é”™è¯¯", exc_info=True)
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æµ‹è¯•å®¹é”™
        print(f"\nğŸ”„ æµ‹è¯•é‡è¯•æœºåˆ¶")
        print("-" * 30)
        
        from llm_cooperation.resilience import RetryHandler
        
        retry_handler = RetryHandler(
            max_retries=3,
            backoff_strategy="exponential",
            base_delay=1.0,
            max_delay=10.0
        )
        
        async def flaky_function():
            """æ¨¡æ‹Ÿéšæœºå¤±è´¥çš„å‡½æ•°"""
            if random.random() < 0.7:  # 70%å¤±è´¥ç‡
                raise APIConnectionError("æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯")
            return "æˆåŠŸ!"
        
        try:
            result = await retry_handler.execute_with_retry(flaky_function)
            print(f"âœ… é‡è¯•æœºåˆ¶æˆåŠŸ: {result}")
        except Exception as e:
            print(f"âŒ é‡è¯•æœºåˆ¶è€—å°½: {e}")
        
        # æµ‹è¯•æ–­è·¯å™¨
        print(f"\nâš¡ æµ‹è¯•æ–­è·¯å™¨")
        print("-" * 30)
        
        from llm_cooperation.resilience import CircuitBreaker
        
        circuit_breaker = CircuitBreaker(
            failure_threshold=3,
            recovery_timeout=10.0,
            expected_exception=APIConnectionError
        )
        
        async def unreliable_service():
            """æ¨¡æ‹Ÿä¸å¯é çš„å¤–éƒ¨æœåŠ¡"""
            if random.random() < 0.8:  # 80%å¤±è´¥ç‡
                raise APIConnectionError("æœåŠ¡ä¸å¯ç”¨")
            return "æœåŠ¡å“åº”"
        
        # æµ‹è¯•æ–­è·¯å™¨è¡Œä¸º
        for i in range(10):
            try:
                result = await circuit_breaker.call(unreliable_service)
                print(f"  è¯·æ±‚ {i+1}: âœ… {result}")
            except CircuitBreakerOpenError:
                print(f"  è¯·æ±‚ {i+1}: ğŸš« æ–­è·¯å™¨å¼€å¯")
            except APIConnectionError as e:
                print(f"  è¯·æ±‚ {i+1}: âŒ {e}")
            
            await asyncio.sleep(0.5)  # è¯·æ±‚é—´çŸ­æš‚å»¶è¿Ÿ
        
        # æµ‹è¯•ä¼˜é›…é™çº§
        print(f"\nğŸ­ æµ‹è¯•ä¼˜é›…é™çº§")
        print("-" * 30)
        
        from llm_cooperation.resilience import GracefulDegradation
        
        degradation_handler = GracefulDegradation(
            fallback_models=["qwen3_8b", "backup_model"],
            cache_enabled=True,
            cache_ttl=300  # 5åˆ†é’Ÿ
        )
        
        # ä¸»æ¨¡å‹å¤±è´¥ï¼Œåº”è¯¥å›é€€
        primary_request = InferenceRequest(
            prompt="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            model_name="primary_model_that_fails"
        )
        
        try:
            response = await degradation_handler.handle_request(primary_request)
            print(f"âœ… ä¼˜é›…é™çº§æˆåŠŸ: ä½¿ç”¨äº† {response.model_name}")
            print(f"   å“åº”: {response.text[:100]}...")
        except Exception as e:
            print(f"âŒ ä¼˜é›…é™çº§å¤±è´¥: {e}")
    
    finally:
        await engine.shutdown()

asyncio.run(error_handling_example())
```

è¿™ä¸ªç»¼åˆç¤ºä¾‹æ–‡æ¡£æä¾›äº†LLMåä½œç³»ç»Ÿçš„å®é™…ã€çœŸå®ä¸–ç•Œä½¿ç”¨æ¨¡å¼ï¼Œæ¶µç›–ä»åŸºç¡€æ¨ç†åˆ°é«˜çº§ä¼ä¸šåº”ç”¨çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬é”™è¯¯å¤„ç†å’Œæ€§èƒ½ç›‘æ§ã€‚

---

**è¯­è¨€é€‰æ‹©**: [English](/) | [ä¸­æ–‡](/zh/)