---
layout: default
title: å¿«é€Ÿå…¥é—¨
nav_order: 2
description: "LLMåä½œç³»ç»Ÿçš„å®‰è£…å’Œé…ç½®æŒ‡å—"
parent: ä¸­æ–‡æ–‡æ¡£
---

# å¿«é€Ÿå…¥é—¨
{: .no_toc }

LLMåä½œç³»ç»Ÿçš„å®Œæ•´å®‰è£…å’Œé…ç½®æŒ‡å—ã€‚
{: .fs-6 .fw-300 }

## ç›®å½•
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## å®‰è£…è¦æ±‚

### ç³»ç»Ÿè¦æ±‚

- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- pip åŒ…ç®¡ç†å™¨
- äº’è”ç½‘è¿æ¥ï¼ˆç”¨äºAPIè®¿é—®ï¼‰

### æ¨èé…ç½®

- **å¼€å‘ç¯å¢ƒ**: Python 3.9+, 4GB RAM
- **ç”Ÿäº§ç¯å¢ƒ**: Python 3.9+, 8GB RAM, SSDå­˜å‚¨
- **å¹¶å‘åœºæ™¯**: 16GB RAM, å¤šæ ¸CPU

## å®‰è£…æ–¹æ³•

### ä»GitHubå®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install git+https://github.com/ZJU-REAL/llm-cooperation.git

# åŒ…å«å¼€å‘ä¾èµ–
pip install "git+https://github.com/ZJU-REAL/llm-cooperation.git[dev]"

# åŒ…å«æœåŠ¡å™¨ç»„ä»¶
pip install "git+https://github.com/ZJU-REAL/llm-cooperation.git[server]"

# å®Œæ•´å®‰è£…ï¼ˆåŒ…å«æ‰€æœ‰åŠŸèƒ½ï¼‰
pip install "git+https://github.com/ZJU-REAL/llm-cooperation.git[all]"
```

### å¼€å‘ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/ZJU-REAL/llm-cooperation.git
cd llm-cooperation

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•éªŒè¯å®‰è£…
pytest tests/

# æ£€æŸ¥ä»£ç è´¨é‡
flake8 llm_cooperation/
black --check llm_cooperation/
```

## åŸºç¡€é…ç½®

### ç¯å¢ƒå˜é‡é…ç½®

åœ¨é¡¹ç›®ç›®å½•åˆ›å»º`.env`æ–‡ä»¶ï¼š

```env
# APIé…ç½®
BASE_URL=https://api2.aigcbest.top/v1
API_KEY=æ‚¨çš„APIå¯†é’¥

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# æœåŠ¡å™¨é…ç½®ï¼ˆå¯é€‰ï¼‰
SERVER_HOST=0.0.0.0
SERVER_PORT=8080

# æ¨¡å‹åå¥½è®¾ç½®ï¼ˆå¯é€‰ï¼‰
DEFAULT_REASONING_MODEL=qwen3_32b
DEFAULT_LIGHTWEIGHT_MODEL=qwen3_8b
DEFAULT_MULTIMODAL_MODEL=qwen2_vl_72b

# æ€§èƒ½é…ç½®
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
```

### ä½¿ç”¨é…ç½®CLIå·¥å…·

ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„CLIå·¥å…·æ¥ç®¡ç†é…ç½®ï¼š

```bash
# AIGC Bestå¿«é€Ÿé…ç½®ï¼ˆæ¨èï¼‰
llm-config preset --name aigcbest --api-key æ‚¨çš„APIå¯†é’¥

# æ·»åŠ è‡ªå®šä¹‰APIç«¯ç‚¹
llm-config add-endpoint \
  --name custom \
  --base-url https://api.example.com/v1 \
  --api-key æ‚¨çš„å¯†é’¥

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹
llm-config add-model \
  --name custom_model \
  --path "provider/model-name" \
  --max-tokens 4096 \
  --tasks "text,code,analysis"

# æµ‹è¯•è¿æ¥
llm-config test

# åˆ—å‡ºæ‰€æœ‰å·²é…ç½®çš„æ¨¡å‹
llm-config list

# æ˜¾ç¤ºå½“å‰é…ç½®
llm-config show

# å¯¼å‡ºé…ç½®åˆ°æ–‡ä»¶
llm-config export --output config.json

# ä»æ–‡ä»¶å¯¼å…¥é…ç½®
llm-config import --input config.json
```

### æ”¯æŒçš„APIæä¾›å•†

ç³»ç»Ÿæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„APIã€‚ä»¥ä¸‹æ˜¯ä¸€äº›çƒ­é—¨æä¾›å•†çš„é…ç½®ï¼š

#### AIGC Bestï¼ˆæ¨èï¼‰
```bash
llm-config preset --name aigcbest --api-key æ‚¨çš„å¯†é’¥
```
- åŸºç¡€URL: `https://api2.aigcbest.top/v1`
- å¯ç”¨æ¨¡å‹: Qwen/Qwen3-32B, Qwen/Qwen3-8B, DeepSeek-V3ç­‰
- ä¼˜åŠ¿: æ€§ä»·æ¯”é«˜ï¼Œæ¨¡å‹ä¸°å¯Œï¼Œç¨³å®šæ€§å¥½

#### OpenAIå®˜æ–¹
```bash
llm-config preset --name openai --api-key æ‚¨çš„å¯†é’¥
```
- åŸºç¡€URL: `https://api.openai.com/v1`
- å¯ç”¨æ¨¡å‹: gpt-4, gpt-3.5-turboç­‰
- ä¼˜åŠ¿: è¡Œä¸šæ ‡æ†ï¼Œè´¨é‡ç¨³å®š

#### DeepSeek
```bash
llm-config preset --name deepseek --api-key æ‚¨çš„å¯†é’¥
```
- åŸºç¡€URL: `https://api.deepseek.com/v1`
- å¯ç”¨æ¨¡å‹: deepseek-chat, deepseek-coderç­‰
- ä¼˜åŠ¿: ä»£ç ç”Ÿæˆèƒ½åŠ›å¼º

#### è‡ªå®šä¹‰æä¾›å•†
```bash
llm-config add-endpoint \
  --name è‡ªå®šä¹‰åç§° \
  --base-url https://æ‚¨çš„API.com/v1 \
  --api-key æ‚¨çš„å¯†é’¥ \
  --models "model1,model2,model3"
```

## åŸºç¡€ä½¿ç”¨

### ç®€å•æ¨ç†ç¤ºä¾‹

```python
import asyncio
from llm_cooperation import SystemConfig, OpenAIEngine, InferenceRequest

async def basic_example():
    # åˆå§‹åŒ–é…ç½®
    config = SystemConfig()
    
    # å¦‚æœæ²¡æœ‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥åœ¨ä»£ç ä¸­é…ç½®
    config.update_api_config(
        base_url="https://api2.aigcbest.top/v1",
        api_key="æ‚¨çš„APIå¯†é’¥"
    )
    
    # åˆ›å»ºå¼•æ“
    engine = OpenAIEngine(config)
    await engine.initialize()
    
    try:
        # åˆ›å»ºè¯·æ±‚
        request = InferenceRequest(
            prompt="ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—",
            model_name="qwen3_8b",
            max_tokens=300,
            temperature=0.7
        )
        
        # è·å–å“åº”
        response = await engine.inference(request)
        
        if response.success:
            print(f"æ¨¡å‹: {response.model_name}")
            print(f"å“åº”: {response.text}")
            print(f"ä½¿ç”¨çš„ä»¤ç‰Œæ•°: {response.usage.get('total_tokens')}")
            print(f"å»¶è¿Ÿ: {response.latency:.2f}ç§’")
        else:
            print(f"é”™è¯¯: {response.error}")
    
    finally:
        await engine.shutdown()

# è¿è¡Œç¤ºä¾‹
asyncio.run(basic_example())
```

### æ™ºèƒ½è·¯ç”±ç¤ºä¾‹

ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨ä¸ºæ‚¨çš„ä»»åŠ¡é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼š

```python
from llm_cooperation import IntelligentRouter

async def routing_example():
    router = IntelligentRouter()
    
    # è·¯ç”±å™¨ä¼šåˆ†ææŸ¥è¯¢å¹¶é€‰æ‹©é€‚å½“çš„æ¨¡å‹
    queries = [
        "2+2ç­‰äºå¤šå°‘ï¼Ÿ",  # ç®€å•æ•°å­¦ -> è½»é‡çº§æ¨¡å‹
        "è¯æ˜å‹¾è‚¡å®šç†",  # å¤æ‚æ•°å­¦ -> æ¨ç†æ¨¡å‹
        "å°†'ä½ å¥½'ç¿»è¯‘æˆè‹±æ–‡",  # ç®€å•ä»»åŠ¡ -> è½»é‡çº§æ¨¡å‹
        "åˆ†æäººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„æ·±è¿œå½±å“",  # å¤æ‚åˆ†æ -> å¯èƒ½ä½¿ç”¨åä½œ
    ]
    
    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        result = await router.route_request(query)
        print(f"ç»“æœ: {result[:100]}...")
        print()
```

### å¤šæ¨¡å‹åä½œç¤ºä¾‹

å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¤šä¸ªæ¨¡å‹ååŒå·¥ä½œï¼š

```python
from llm_cooperation import CooperationScheduler

async def cooperation_example():
    scheduler = CooperationScheduler()
    
    complex_task = """
    åˆ†æå¯å†ç”Ÿèƒ½æºé‡‡ç”¨çš„åˆ©å¼Šã€‚
    è€ƒè™‘ç»æµã€ç¯å¢ƒå’ŒæŠ€æœ¯å› ç´ ã€‚
    ä¸ºæ”¿ç­–åˆ¶å®šè€…æä¾›å…·ä½“å»ºè®®ã€‚
    """
    
    # é¡ºåºåä½œï¼šæ¨¡å‹ç›¸äº’åŸºäºå½¼æ­¤çš„å·¥ä½œ
    print("ğŸ“‹ é¡ºåºåä½œ:")
    result = await scheduler.create_sequential_task(
        query=complex_task,
        models=["qwen3_32b", "qwen3_8b"],
        integration_strategy="ensemble"
    )
    print(f"ç»“æœ: {result[:200]}...")
    
    # å¹¶è¡Œåä½œï¼šæ¨¡å‹ç‹¬ç«‹å·¥ä½œç„¶åæ•´åˆ
    print("\nğŸ“‹ å¹¶è¡Œåä½œ:")
    result = await scheduler.create_parallel_task(
        query=complex_task,
        models=["qwen3_32b", "qwen3_8b"],
        integration_strategy="voting"
    )
    print(f"ç»“æœ: {result[:200]}...")
```

## å‘½ä»¤è¡Œç•Œé¢

### å¯åŠ¨æœåŠ¡å™¨

```bash
# ä½¿ç”¨é»˜è®¤è®¾ç½®å¯åŠ¨
llm-cooperation server

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®å¯åŠ¨
llm-cooperation server --host 0.0.0.0 --port 8080 --config /path/to/config.json

# ä½¿ç”¨è°ƒè¯•æ—¥å¿—å¯åŠ¨
llm-cooperation server --log-level DEBUG

# åå°è¿è¡Œ
nohup llm-cooperation server > server.log 2>&1 &
```

### CLIå‘½ä»¤

```bash
# å•æ¬¡æ¨ç†æµ‹è¯•
llm-cooperation infer --prompt "ä»€ä¹ˆæ˜¯AIï¼Ÿ" --model qwen3_8b

# æ€§èƒ½åŸºå‡†æµ‹è¯•
llm-cooperation benchmark --model qwen3_8b --requests 10 --concurrent 2

# å¥åº·æ£€æŸ¥
llm-cooperation health

# æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
llm-cooperation status

# é…ç½®ç®¡ç†
llm-cooperation config show
llm-cooperation config test
llm-cooperation config list-models

# æ—¥å¿—æŸ¥çœ‹
llm-cooperation logs --tail 100 --follow
```

## é…ç½®éªŒè¯

### æµ‹è¯•æ‚¨çš„è®¾ç½®

```python
import asyncio
from llm_cooperation.tools import APIConfigManager

async def test_setup():
    manager = APIConfigManager()
    
    # æµ‹è¯•æ¨¡å‹è¿æ¥
    print("ğŸ” æµ‹è¯•æ¨¡å‹è¿æ¥...")
    results = await manager.test_model_connectivity()
    
    for model, result in results.items():
        status = "âœ…" if result['accessible'] else "âŒ"
        latency = f"{result.get('latency', 'N/A')}ms"
        print(f"{status} {model}: {latency}")
    
    # æµ‹è¯•æ¨ç†åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½...")
    from llm_cooperation import SystemConfig, OpenAIEngine, InferenceRequest
    
    config = SystemConfig()
    engine = OpenAIEngine(config)
    await engine.initialize()
    
    request = InferenceRequest(
        prompt="ä½ å¥½ï¼Œä¸–ç•Œï¼",
        model_name="qwen3_8b",
        max_tokens=50
    )
    
    response = await engine.inference(request)
    
    if response.success:
        print("âœ… è®¾ç½®éªŒè¯æˆåŠŸï¼")
        print(f"æµ‹è¯•å“åº”: {response.text[:100]}...")
    else:
        print(f"âŒ è®¾ç½®éªŒè¯å¤±è´¥: {response.error}")
    
    await engine.shutdown()

asyncio.run(test_setup())
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

åˆ›å»º`config.json`é…ç½®æ–‡ä»¶ï¼š

```json
{
  "api_config": {
    "base_url": "https://api2.aigcbest.top/v1",
    "api_key": "æ‚¨çš„APIå¯†é’¥",
    "timeout": 30,
    "max_retries": 3
  },
  "models": {
    "qwen3_32b": {
      "model_path": "Qwen/Qwen3-32B",
      "max_tokens": 4096,
      "supported_tasks": ["reasoning", "analysis", "code"],
      "temperature": 0.7
    },
    "qwen3_8b": {
      "model_path": "Qwen/Qwen3-8B", 
      "max_tokens": 2048,
      "supported_tasks": ["text", "translation", "simple"],
      "temperature": 0.7
    }
  },
  "routing": {
    "default_strategy": "intelligent",
    "complexity_threshold": 0.6,
    "fallback_model": "qwen3_8b"
  },
  "cooperation": {
    "max_parallel_models": 3,
    "integration_timeout": 60,
    "consensus_threshold": 0.7
  },
  "monitoring": {
    "enabled": true,
    "metrics_interval": 60,
    "health_check_interval": 30
  }
}
```

## å¸¸è§é—®é¢˜è§£å†³

### å¸¸è§é—®é¢˜

**1. å¯¼å…¥é”™è¯¯**
```bash
ModuleNotFoundError: No module named 'llm_cooperation'
```
è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿åŒ…å·²å®‰è£…ï¼š`pip install git+https://github.com/ZJU-REAL/llm-cooperation.git`

**2. APIè¿æ¥é”™è¯¯**
```bash
Connection failed: Invalid API key or endpoint
```
è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥æ‚¨çš„APIå¯†é’¥å’ŒåŸºç¡€URLé…ç½®ï¼š
```bash
llm-config test
llm-config show
```

**3. æ¨¡å‹æœªæ‰¾åˆ°**
```bash
Model 'custom_model' not found in configuration
```
è§£å†³æ–¹æ¡ˆï¼šå°†æ¨¡å‹æ·»åŠ åˆ°é…ç½®ä¸­ï¼š
```bash
llm-config add-model --name custom_model --path "provider/model-name"
```

**4. å†…å­˜ä¸è¶³**
```bash
OutOfMemoryError: Cannot allocate memory
```
è§£å†³æ–¹æ¡ˆï¼š
- å‡å°‘`max_concurrent_requests`
- é™ä½`max_tokens`è®¾ç½®
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹è¿›è¡Œç®€å•ä»»åŠ¡

**5. ç½‘ç»œè¶…æ—¶**
```bash
TimeoutError: Request timed out
```
è§£å†³æ–¹æ¡ˆï¼š
- å¢åŠ `REQUEST_TIMEOUT`è®¾ç½®
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨æ›´å¿«çš„APIç«¯ç‚¹

### è°ƒè¯•æŠ€å·§

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
try:
    response = await engine.inference(request)
except Exception as e:
    logging.error(f"æ¨ç†å¤±è´¥: {e}", exc_info=True)
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ¨¡å‹é€‰æ‹©ä¼˜åŒ–**
   - ç®€å•ä»»åŠ¡ä½¿ç”¨å°æ¨¡å‹
   - å¤æ‚ä»»åŠ¡ä½¿ç”¨å¤§æ¨¡å‹
   - å¯ç”¨æ™ºèƒ½è·¯ç”±

2. **å¹¶å‘è®¾ç½®**
   - æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´å¹¶å‘æ•°
   - ç›‘æ§å†…å­˜å’ŒCPUä½¿ç”¨ç‡
   - ä½¿ç”¨è¿æ¥æ± 

3. **ç¼“å­˜ç­–ç•¥**
   - å¯ç”¨å“åº”ç¼“å­˜
   - è®¾ç½®åˆç†çš„ç¼“å­˜è¿‡æœŸæ—¶é—´
   - ä½¿ç”¨Redisè¿›è¡Œåˆ†å¸ƒå¼ç¼“å­˜

## ä¸‹ä¸€æ­¥

- æ¢ç´¢[ä½¿ç”¨ç¤ºä¾‹](examples.md)äº†è§£é«˜çº§ç”¨æ³•
- é˜…è¯»[APIå‚è€ƒ](api-reference.md)è·å–å®Œæ•´æ–‡æ¡£
- æŸ¥çœ‹[English documentation](../)è·å–æ›´å¤šèµ„æº

---

**è¯­è¨€é€‰æ‹©**: [English](/) | [ä¸­æ–‡](/zh/)